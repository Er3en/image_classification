{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from progressbar import *\n",
    "from rich.progress import track\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "from dataset import Classification_Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from transforms import *\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import hydra\n",
    "import numpy as np\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from torch import nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "from torchvision import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'cane': 'dog',\n",
    "'cavallo': 'horse',\n",
    "'elefante': 'elephant',\n",
    "'farfalla': 'butterfly',\n",
    "'gallina': 'chicken',\n",
    "'gatto': 'cat',\n",
    "'mucca': 'cow',\n",
    "'pecora': 'sheep',\n",
    "'ragno': 'spider',\n",
    "'scoiattolo': 'squirrel',\n",
    "}\n",
    "\n",
    "config = {  'epochs': 100,\n",
    "            'lr': 0.0008,\n",
    "            'path': '/home/jarybski/Desktop/Animal-10/archive/raw-img',\n",
    "            'img_size': 256,\n",
    "            'batch_size': 8,\n",
    "            'shuffle': True,\n",
    "            'pin_memory': False,\n",
    "            'random_state':192,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "def transform_train(cfg):\n",
    "    data_transform = transforms.Compose([transforms.Resize(224),\n",
    "                                         transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                         transforms.RandomVerticalFlip(p=0.5),\n",
    "                                         transforms.Normalize(mean=255,std=1),\n",
    "                                         transforms.ToTensor(),\n",
    "                                        ])\n",
    "    return data_transform\n",
    "    \n",
    "def transform_val(cfg):\n",
    "    data_transform = transforms.Compose([transforms.Resize(cfg.dataset.img_size),\n",
    "                                         transforms.Resize(224),\n",
    "                                         transforms.ToTensor()\n",
    "                                        ])\n",
    "    return data_transform\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_labels():\n",
    "    labels = {'cane': 'dog',\n",
    "    'cavallo': 'horse',\n",
    "    'elefante': 'elephant',\n",
    "    'farfalla': 'butterfly',\n",
    "    'gallina': 'chicken',\n",
    "    'gatto': 'cat',\n",
    "    'mucca': 'cow',\n",
    "    'pecora': 'sheep',\n",
    "    'ragno': 'spider',\n",
    "    'scoiattolo': 'squirrel',\n",
    "    }\n",
    "    return labels\n",
    "\n",
    "def create_df():\n",
    "    cfg = OmegaConf.load(\"cfg/config.yaml\")\n",
    "    cfg_ds = cfg.dataset\n",
    "    data = {'path': [],'label': []}\n",
    "    labels = get_labels()\n",
    "\n",
    "    for  dir in os.listdir(cfg_ds.path):\n",
    "        filename = os.listdir(f'{cfg_ds.path}/{dir}')    \n",
    "        for file in filename:\n",
    "            data['path'].append(f'{cfg_ds.path}/{dir}/{file}')\n",
    "            data['label'].append(labels[dir])\n",
    "    df_data = pd.DataFrame(data)\n",
    "    return df_data, labels\n",
    "\n",
    "\n",
    "def create_dataloaders():\n",
    "    cfg = OmegaConf.load(\"cfg/config.yaml\")\n",
    "\n",
    "    cfg_ds = cfg.dataset\n",
    "    df, labels = create_df()\n",
    "    df_train, df_valid = train_test_split(df, test_size=0.2, random_state=cfg.dataset.random_state, stratify=df['label'])\n",
    "    \n",
    "    train_dataset = Classification_Dataset(dataframe=df_train,labels=labels, shuffle=True, transforms=transform_train(cfg))\n",
    "    print(len(train_dataset))\n",
    "    print(train_dataset.__sizeof__())\n",
    "    val_dataset = Classification_Dataset(dataframe=df_valid,labels=labels, shuffle=True, transforms=transform_val(cfg))\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=cfg_ds.batch_size, \n",
    "        shuffle=cfg_ds.shuffle, \n",
    "        num_workers=cfg_ds.num_workers,\n",
    "        pin_memory=cfg_ds.pin_mem\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=cfg_ds.batch_size,                       \n",
    "        shuffle=cfg_ds.shuffle, \n",
    "        num_workers=cfg_ds.num_workers,\n",
    "        pin_memory=cfg_ds.pin_mem\n",
    "        )\n",
    "\n",
    "    return train_loader, val_loader, labels, train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(df):\n",
    "    plt.figure(figsize = (20, 6))\n",
    "    for idx, i in enumerate(df.label.unique()):\n",
    "        plt.subplot(1, 10, idx + 1)\n",
    "        df = df[df['label'] == i].reset_index(drop=True)\n",
    "        image_path = df.loc[random.randint(0, len(df) - 1), 'path']\n",
    "        img = Image.open(image_path)\n",
    "        img = img.resize((224,224))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(i)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20902\n",
      "32\n",
      "Train loader <torch.utils.data.dataloader.DataLoader object at 0x7f45f0f3e380>\n",
      "Validation loader <torch.utils.data.dataloader.DataLoader object at 0x7f45f0f3ed10>\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, labels, train_dataset = create_dataloaders()\n",
    "print(\"Train loader\", train_loader)\n",
    "print(\"Validation loader\", valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['horse']['butterfly']\n",
      "\n",
      "[0][6]\n",
      "['dog']\n",
      "\n",
      "[4]\n",
      "['squirrel']\n",
      "[9]\n",
      "['spider']['spider']\n",
      "\n",
      "[8][8]\n",
      "\n",
      "['butterfly']\n",
      "[0]['sheep']\n",
      "\n",
      "[7]\n",
      "['chicken']['dog']\n",
      "\n",
      "[4]\n",
      "['spider']\n",
      "[2]\n",
      "[8]\n",
      "['cat']\n",
      "['dog'][1]\n",
      "['squirrel']\n",
      "[4]\n",
      "\n",
      "[9]['spider']\n",
      "['cow']\n",
      "[8]\n",
      "\n",
      "[3]['squirrel']\n",
      "\n",
      "[9]\n",
      "['chicken']\n",
      "[2]['cow']\n",
      "[3]\n",
      "\n",
      "['cow']['butterfly']\n",
      "\n",
      "\n",
      "[0]['elephant'][3]\n",
      "\n",
      "['squirrel']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jarybski/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jarybski/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/home/jarybski/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 160, in default_collate\n    return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})\n  File \"/home/jarybski/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 160, in <dictcomp>\n    return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})\n  File \"/home/jarybski/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 149, in default_collate\n    return default_collate([torch.as_tensor(b) for b in batch])\n  File \"/home/jarybski/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 141, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [240, 300, 3] at entry 0 and [300, 279, 3] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [154], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# for i in range(10):\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     item = train_dataset.__getitem__(i)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#     print(type(item))\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# a = torch.FloatTensor(item,224,224)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# a = F.to_pil_image(a)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#print(item.size())\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idxc, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):    \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(data))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1376\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1375\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1376\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1402\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1403\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:461\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 461\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jarybski/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jarybski/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/home/jarybski/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 160, in default_collate\n    return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})\n  File \"/home/jarybski/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 160, in <dictcomp>\n    return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})\n  File \"/home/jarybski/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 149, in default_collate\n    return default_collate([torch.as_tensor(b) for b in batch])\n  File \"/home/jarybski/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 141, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [240, 300, 3] at entry 0 and [300, 279, 3] at entry 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "# for i in range(10):\n",
    "#     item = train_dataset.__getitem__(i)\n",
    "#     print(type(item))\n",
    "    # a = torch.FloatTensor(item,224,224)\n",
    "    # a = F.to_pil_image(a)\n",
    "#print(item.size())\n",
    "for batch_idxc, data in enumerate(train_loader):    \n",
    "    print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
